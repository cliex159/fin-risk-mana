# (PART) Chapter 2 Univariate Volatility {.unnumbered}

# Stationary processes

The volatility plays a crucial role in financial risk management, it is
the main measure of risk. On the other hands, the volatility is the
key factor in, e.g., Investment decisions, Portfolio construction
(Markowitz model) and Derivative pricing (Black-Scholes model).

* In this Chapter we focus on the estimation and forecasting of
volatility for a single asset (univariate).
* The volatility plays a crucial role in financial risk management, it is
the main measure of risk. On the other hands, the volatility is the
key factor in, e.g., Investment decisions, Portfolio construction
(Markowitz model) and Derivative pricing (Black-Scholes model).
* In this Chapter we focus on the estimation and forecasting of
volatility for a single asset (univariate).

## Time series

* A time series is a sequence of observations in chronological order. For example: daily log returns on a stock or monthly values of the Consumer Price Index (CPI).
* A stochastic process is a sequence of random variables and can be viewed as the “theoretical” or “population” analog of a time series, conversely, a time series can be considered a sample from a stochastic process.
* Denote $\{X_t, t \in I\}$ the time series, where I is a time index. For example: $I = \{1, 2, 3, ...\}$ or $I = \{2000, 2001, 2002...2021\}$. Equally spaced time series are the most common in practice. This is the case of
$I = \{t_1, t_2, ..., t_n\}$, where
$(\Delta = t_{i+1} − t)_i$ with $\Delta$ is a constant.

### Remark

**Difference from traditional Statistical Inference**

* In traditional statistic inference, the data is assumed to be an i.i.d
process (random sample).
* In time series, we do not need this assumption and wish to model the dependency among observations which leads to the concept of autocorrelation.

**Some main problems in time series**

* Formulate and estimate a parametric model for $X_t$ (need to propose methods of estimation and model diagnostics).
* This point is related to the estimation of autoregressive (AR) or ARMA models.
* Estimation of Missing values (fill“gaps”).
* Prediction or Forecasting (“would like to know what a future value is”). For example our data is $x_1, x_2, ..., x_{100}$, we wish to forecast the next 10 values, $x_{101}, ..., x_{110}$. In this case, our forecasting horizon is 10.
* Plotting time series to observe fluctuations of time series, e.g., to find stationarity or non-stationarity, cycles, trends, outliers or interventions. Assisting in the formulation of a parametric model.

### Example {.unnumbered}

Consider Financial Index SP500. The data consists of excess returns $X_t = \log(S_t) −\log(S_{t−1})$. From the plot we see the following properties of $X_t$:

```{r,message=FALSE,warning=FALSE}
sp500=read.csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vT4WqdVoUIiaMcd4jQj5by3Oauc6G4EFq9VDDrpzG2oBn6TFzyNE1yPV2fKRal5F7DmRzCtVa4nSQIw/pub?gid=279168786&single=true&output=csv")
plot(diff(log(sp500$Close)),type="l",col="blue")
```

* The mean level of the process seems constant.
* There are sections of the data with explosive behavior (high volatility).
* The data corresponds to a non-stationary process (will define more detailed).
* The variance (or volatility) is not constant in time.
* No linear time series model will be available for this data.

## Autocovariance

### Definition

The autocovariance function a stochastic process $X$ is defined as 
$$\gamma(t,\tau)=\mathbb{E}(X_t −\mu_t)(X_{t−\tau} −\mu_{t−\tau})$$
for $\tau \in \mathbb{Z}$, where $\mu_t = E(X_t)$.

* The autocovariance function is symmetric, i.e., $\gamma(t,\tau) = \gamma(t − \tau,−\tau)$. For special case $\tau = 0$ then $\gamma(t, 0) = Var(X_t)$.
* In general $\gamma(t,\tau)$ is depend on t as well as $\tau$.

### Example {.unnumbered}

Find the autocovariance function of Brownian motion?

>$$\begin{align*}
&B_t \sim \mathcal{N}(0,t) \\
&\rightarrow E[B_t^2] =Var(B_t)=t
\end{align*}$$

>$$\begin{align*}
&B_t-B_{t-\tau} \sim \mathcal{N}(0,t-\tau) \\
&\rightarrow E[(B_t-B_{t-\tau})^2]=Var(B_t-B_{t-\tau})=t-\tau
\end{align*}$$

>$$\begin{align*}
\gamma(t,\tau)&=E[(B_t-\mu_t)(B_{t-\tau}-\mu_{t-\tau})] \\
&=E[B_tB_{t-\tau}] \\
&=-\frac{1}{2}E[(B_t-B_{t-\tau})^2-B_t^2-B_{t-\tau}^2] \\
&=-\frac{1}{2}\{E[(B_t-B_{t-\tau})^2] -E[B_t^2]-E[B_{t-\tau}^2]\} \\
&=-\frac{1}{2} [(\tau)-(t)-(t-\tau)]=t-\tau
\end{align*}$$

>Answer: The autocovariance function of Brownian motion is $t-\tau$.

##  Stationary

### Strictly Stationary

A process is said to be strictly stationary if all aspects of its behavior are unchanged by shifts in time. Mathematically, stationarity is defined as the requirement that for every $m$ and $n$ the distribution of
$(X_1, X_2, ..., X_n)$ and $(X_{1+m}, X_{2+m}, ..., X_{n+m})$ are the same.

### Weakly Stationary

A process is weakly stationary if its mean, variance, and covariance are unchanged by time shifts. More precisely, $X_1, X_2, ...,$ is a weakly stationary process if

1. $\mathbb{E}(X_t)=\mu, \forall t$
2. $Var(X_t) = \sigma_2$ (a positive finite constant) for all $t$.
3. $Cov(X_t, X_s) = \gamma(|t − s|), \forall t, s$ and some function $\gamma$.

We see that, the mean and variance do not change with time and the covariance between two observations depends only on the lag, the time distance $|t − s|$.

* The function $\gamma$ is the autocovariance function of the process and has symmetric property $\gamma(h) = \gamma(−h)$.

$$\begin{align*}
\gamma(h)=cov(X_t,X_{t+h}) \\
\rightarrow \gamma(-h)=cov(X_{t},X_{t-h})
\end{align*}$$

Let $s=t-h$ then $t=s+h$
$$\gamma(-h)=cov(X_{s+h},X_{s})=\gamma(h) $$

* The correlation between $X_t$ and $X_{t+h}$ is denoted by $\rho(h)$. Function $\rho$ is called autocorrelation function (ACF). We have $\gamma(0) = \sigma^2$ and, hence
$\gamma(h) = \sigma^2 \rho(h)$ hence $\rho(h) = \frac{\gamma(h)}{\gamma(0)}$.

The ACF is normalized on $[−1, 1]$. Since the process is required to be covariance stationary, the ACF depends only on one parameter, lag $h$.

### Example {.unnumbered}

Consider the random walk $X: X_t = c + X_{t−1} + \epsilon_t$, with c is constant and white noise $\epsilon_t$. We see that if $c \neq 0$, then $Z_t := X_t −X_{t−1} = c+ \epsilon_t$ have a non-zero mean. We call it a random walk with drift. Note that since $\epsilon_t$ is independent then we call $X_t$ a random walk with independent increments. For more convenience, assume that $c$ and $X_0$ are set to zero. We have 

$$\begin{align*}
&X_t =\epsilon_t + \epsilon_{t−1} +...+\epsilon_1 \\
\\
&\mu_t =E(X_t)=0 \\
\\
&Var(X_t) = t\sigma
\end{align*}$$

$Var(X_t)$ is not stationary but rather increases linearly with time and makes the random walk “wander”, i.e., $X_t$ takes increasingly longer excursions away from its conditional mean of $0$, and therefore is not mean-reverting.

If $s<t$ then

$$ \rho(t,s)=\sqrt{1-\frac{s}{t}} $$

which against $\rho$ depending on $t$ as well as on $s$, thus the random walk is not covariance stationary. The following figure shows the relationship among different processes: Stationary processes are the largest set, followed by white noise, martingale difference (MD), and i.i.d. processes.

<center>
![](/Users/cliex159/Documents/fin-risk-mana/Frisk/21.png)
</center>

## Estimating Parameters

Let $X_1, X_2, ..., X_n$ be observations from weakly stationary process. To estimate the autocovariance function, we use the sample autocovariance function defined by

$$ \hat{\gamma}(h)=\frac{1}{n} \sum_{t=1}^{n-h}(X_{t+h}-\bar X)(X-t-\bar X) $$

To estimate function $\rho$, we use the sample autocorrelation function
(sample ACF) defined as

$$\hat \rho(h) =\frac{\hat \gamma(h)}{\hat \gamma(h)}$$

* To visualize the dependencies of $x_t$ for different lags h, we use the Correlogram.
* A correlogram is a plot of $h$ (x-axis) versus its corresponding value of $\hat \rho(h)$ (y-axis).
* The correlogram may exhibit patterns and different degrees of dependency in a time series.
* A “band” of size $\frac{2}{\sqrt{n}}$ is added to the correlogram because asymptotically $\hat \rho(h) \sim \mathcal{N} \left(0, \frac{1}{n} \right)$ if the data is close to a white noise process.
* This band is used to detect significant autocorrelations, i.e. autocorelations that are different from zero.

```{r,message=FALSE,warning=FALSE}
library(tidyquant)
msft <- tq_get('MSFT',from=as.Date("2010-01-01"),
               to=as.Date("2014-01-01"),
               get = "stock.prices")

msft_logret=msft$adjusted %>% 
  log() %>% 
  diff()

acf(msft_logret,lag.max=10)
```

## The ADF Test

ADF Test is also called Unit Root Test. The test uses the following null and alternative hypotheses:

* $H_0$ : The time series contains a unit root. This means that the time series is non-stationary, i.e., it has some time-dependent structure and does not have constant variance over time.
* $H_1$ : The time series is stationary.

```{r,warning=FALSE,message=FALSE}
library(tseries)
adf.test(msft_logret)
```

## KPSS test

The ideas of KPSS test comes from the regression model with time
  trend

$X_t =c+ \mu_t+k \sum_{i=1}^{t} \xi_i +\eta_t$

with stationary $\eta_t$ and i.i.d $\xi$ with mean $0$ and variance $1$. Note that the third term is a random walk. So we set the null hypothesis: the data is stationary and.

$$ H_0 : k = 0 \\
H_1 : k \neq 0 $$

Test results for Microsoft data

```{r,message=FALSE,warning=FALSE}
library(tseries)
kpss.test(msft_logret)
```

##  Ljung–Box Test

Sample ACF with test bounds.

* These bounds are used to test the null hypothesis that an autocorrelation coefficient is $0$. 
* The null hypothesis is rejected if the sample autocorrelation is outside the bounds.
* The usual level of the test is $0.05$.

### Example {.unnumbered}

(The First-order Autoregression Model (AR(1))) The time series $X = (X_t)$ is called AR(1) if the value of X at time t is a linear function of the value of $X$ at time $t − 1$ as follows

$$X_t=\delta+\phi_1 X_{t-1}+w_t=\delta+\sum_{h=0}^\infty \phi_1^h w_{t-h} $$

1. The errors $w_t \sim \mathcal{N}(0,\sigma_w^2)$ are i.i.d.
2. $w_t$ is independent of $X_t$.
3. $\phi_1<1$. This condition guarantees that $X_t$ is weakly stationary.

$$\begin{align*} 
&\mu=\mathbb{E}(X_t)=\frac{\delta}{1-\phi_1} \\
\\
&Var(X_t)=\frac{\sigma_w^2}{1-\phi_1^2} \\
\\
&Cov(X_t,X_{t+h})=\gamma(h)=\phi_1^h \times \frac{\sigma_w^2}{1-\phi_1^2} \\
\\
&\rho(h)=\phi_1^h
\end{align*}$$

Note that the magnitude of its ACF decays geometrically to zero, either slowly as when $\phi_1 = 0.95$, moderately slowly as when $\phi_1 = 0.75$, or rapidly as when $\phi_1 = 0.25$. We now simulate AR(1) and plot the ACF with $\phi_1 =0.64$ and $\sigma_w^2 =1$.

```{r,message=FALSE,warning=FALSE}
library(stats)
ts.sim <- arima.sim(list(order = c(1,0,0), ar = 0.64), n = 100,sd=1)
plot(ts.sim,col="blue")
acf(ts.sim)
```

The null hypothesis of the Ljung–Box test is
$$H_0 :\rho(1)=\rho(2)=...\rho(m)=0$$
for some m. If the Ljung–Box test rejects, then we conclude that one or more of $\rho(1), \rho(2), ..., \rho(m)$ is nonzero. The Ljung–Box test is sometimes called simply the Box test.
$$Q(m)=n(n+2) \sum_{i=j}^m \frac{\hat p^2 (j)}{n-j} \sim \chi^2(m)$$

### Example {.unnumbered}

Consider AR(1) with $\phi_1 = 0.64$ and $\sigma_w^2 = 1$, we have the results of Box test in R

```{r,message=FALSE,warning=FALSE}
library(stats)
ts.sim <- arima.sim(list(order = c(1,0,0), ar = 0.64), n = 100,sd=1)
plot(ts.sim,col="blue")
Box.test(ts.sim, lag = 10, type = "Ljung-Box")
```

If $|\phi_1| \geq 1$ then AR(1) process is nonstationary, and the mean, variance, covariances and and correlations are not constant.

##  PACF

A partial correlation is a conditional correlation. It is the correlation between two variables under the assumption that we know and take into account the values of some other set of variables.

### Example {.unnumbered}

Consider regression model in which $y$ is the response variable, $x_1, x_2, x_3$ are predictor variables. The partial correlation between y and $x_3$ is the correlation between the variables determined taking into account how both $y$ and $x_3$ are related to $x_1$ and $x_2$.

In regression, this partial correlation could be found by correlating the residuals from two different regressions:

1. Regression in which we predict $y$ from $x_1$ and $x_2$. 
2. Regression in which we predict $x_3$ from $x_1$ and $x_2$.

We correlate the “parts” of $y$ and $x_3$ that are not predicted by $x_1$ and $x_2$. We can define the partial correlation just described as
$$\frac{Cov(y, x_3 | x_1, x_2)}{\sqrt{Var(y | x_1, x_2)Var(x_3 | x_1, x_2)}}$$

For a time series, the partial autocorrelation between $x_t$ and $x_{t−h}$ is defined as the conditional correlation between $x_t$ and $x_{t−h}$ conditional on $x_{t−h+1}, ..., x_{t−1}$, the set of observations that come between the time points $t$ and $t − h$.

$$\frac{Cov(y, x_3 | x_1, x_2)}{\sqrt{Var(y | x_1, x_2)Var(x_3 | x_1, x_2)}}$$

### Example {.unnumbered}

The 3rd order (lag) partial autocorrelation is:

$$\frac{Cov(x_t, x_{t-3} | x_{t-1}, x_{t-2})}{\sqrt{Var(x_t | x_t, x_{t-3})Var(x_{t-3} | x_t, x_{t-3})}}$$

# EWMA

Denote $y_t$ the return of stock at time $t$. Then

* Volatility a weighted sum of past returns, with weights $\omega_i$, is
defined by
$$ \hat \sigma_t^2=\omega_1y_{t-1}^2+\omega_2y_{t-2}^2+...+\omega_Ly_{t-L}^2 $$
where L is the length of the estimation window, i.e., the number of observations used in the calculation. This is called MA model.
* An extension of MA model is Exponentially weighted moving average. Let the weights be exponentially declining, and denote them by $\lambda^i$
$$ \hat \sigma_t^2=\lambda y_{t-1}^2+\lambda^2 y_{t-2}^2+...+\lambda^L y_{t-L}^2 $$
where $0 < \lambda < 1$. If $L$ is large enough, the term αn are negligible for all $n > L$. So we set $L = \infty$.
* Note that the sum of weights is
$$\frac{\lambda}{1-\lambda}=\sum_{i=1}^\infty \lambda^i$$
So the exponentially weighted moving average is defined by
$$ \hat \sigma_t^2=\frac{1-\lambda}{\lambda} \sum_{i=1}^{\infty}\lambda^i y_{t-i}^2 $$
and, hence, we get the EWMA equation (why???)
$$ \hat \sigma_t^2=\lambda \hat \sigma_{t-1}^2+(1-\lambda)y_{t-1}^2 $$
* Note that JP Morgan set for daily data with $\lambda = 0.94$.

## Example {.unnumbered}

Suppose that $\lambda = 0.9$, the volatility estimated for a market variable for
day $n − 1$ is $1\%$ per day, and during day $n − 1$ the market variable
increased by $2\%$. This means that $\sigma_{n-1}^2=0.01^2=0.0001$ and $y_{n-1}^2=0.02^2=0.0004$. From equation (1) we get
$$\sigma_n^2=0.9 \times 0.0001 + 0.1 \times 0.0004=0.00013 $$
The estimate of the volatility for day $n$ is $\sigma_n = \sqrt{0.00013} = 1.4\%$ per
   day. Note that the expected value of $y_{n-1}^2$ is $\sigma_{n-1}^2= 0.0001$. Hence, realized value of $y_{n−1}^2 = 0.0002$ is greater that expected value, and as a
result our volatility estimate increase. If the realized value of $y_{n−1}^2$ has been less than its expected valued, our estimate of the volatility would have decreased.

# ARCH and GARCH

## ARCH

* The ARCH model was proposed by Robert Engle in 1982 called autoregressive conditionally heteroscadastic.
* Most volatility models derive from this.
* Returns are assumed to have conditional distribution (here
assumed to be normal)
$$y_t \sim \mathcal{N} (0,\sigma_t^2)$$
or we can write
$$y_t=\sigma_t \epsilon_t $$
where $\epsilon_t \sim \mathcal{N}(0, 1)$ is called residual.

ARCH(L1) is defined by
$$Var(y_t | y_{t−1}, y_{t−2}, ..., y_{t−L_1} ) = \sigma_t^2 = \omega + \sum_{i=1}^{L_1} \alpha_i y_{t−i}^2$$
where $L_1$ is called the lag of the model. It is seen that in the ARCH model, the volatility is weighted average of past returns.  

The most common form is ARCH (1)
$$Var(y_t | y_{t−1}) = \sigma_t^2 = \omega + \alpha y_{t−1}^2$$
where $\omega$ and $\alpha$ are parameters that can be estimated by maximum likelihood.

If we assume that the series has $mean = 0$ (this can always be done by centering), then the ARCH model could be written as
$$\begin{align*}
&y_t = \sigma_t \epsilon_t \\
&\sigma_t=\sqrt{\omega+\alpha y_{t-1}^2} \\
&\epsilon_t \sim \mathcal{N}(0,1),i.i.d
\end{align*}$$

We require that $\omega,\alpha>0$ so that $\omega+\alpha y_{t-1}^2>0, \forall t$. We also require that $\alpha < 1$ in order to the process to be stationary with a finite variance. Now we have
$$y_t^2 =\epsilon_t^2(\omega+\alpha y_{t−1}^2)$$
which is similar to an AR(1) for variable $y_t^2$ and with multiplicative noise with a mean of $1$ rather than additive noise with a mean of $0$.

## GARCH

* It turns out that ARCH model is not a very good model and almost nobody uses it. Because, it needs to use information from many days before t to calculate volatility on day t. That is, it needs a lot of lags.
* The $GARCH(L_1, L_2)$ model is defines as
$$ \sigma_t^2=\omega+\sum_{i=1}^{L_1} \alpha_i y_{t-i}^2 + \sum_{i=1}^{L_2} \beta_i \sigma_{t-i}^2 $$ 
and, hence, $GARCH(1,1)$
$$ \sigma_t^2=\omega+\alpha y_{t-1}^2+\beta \sigma_{t-1}^2 $$
* $GARCH(1,1)$ is the most common specification.

### Unconditional volatility

* The unconditional volatility (so-called the long-run variance rate) is the unconditional expectation of volatility on given time
$$\sigma^2=\mathbb{E}(\sigma_t^2) $$
so we have
$$ \sigma^2=\mathbb{E}(\omega+\alpha y_{t-1}^2+\beta \sigma_{t-1}^2)=\omega +\alpha \sigma^2+\beta \sigma^2 $$
Hence,
$$ \sigma^2=\frac{\omega}{1-\alpha-\beta} $$
* So to ensure positive volatility forecasts we need the condition $\omega, \alpha, \beta \geq 0$
Because if any parameter is negative $\sigma_{t+1}$ may be negative.
* For stationary we need condition $\alpha+\beta<1$
Setting $\gamma := 1 − \alpha − \beta$ and $V := \sigma^2$ (called long-run variance rate). We have
$$ \sigma_t^2=\gamma V+\alpha y_{t-1}^2+\beta \sigma_{t-1}^2 $$

### Meaning of Parameters

* The parameter $\alpha$ is news, it shows that how the volatility reacts to new information.
* The parameter $\beta$ is memory, it shows that how much volatility remembers from the past.
* The sum $\alpha + \beta$ determines how quickly the predictability (memory) of the process dies out:
<ul>
<li> if $\alpha + \beta \approx 0$ predictability will die out very quickly.</li> 
<li> if $\alpha + \beta \approx 1$ predictability will die out very slowly.</li>
</ul>

#### Example {.unnumbered}

Suppose that a $GARCH(1,1)$ model is estimated from daily data is 
$$\sigma_n =0.000002+0.13y_{n-1}^2 +0.86 \sigma_{n-1}^2$$
This corresponds to $\omega = 0.000002, \alpha = 0.13, \beta = 0.86$. We have
$$\sigma^2 = \frac{\omega}{1-\alpha-\beta}= 0.0002$$
or $\sigma=\sqrt{0.0002}=0.014=1.4\%$ per day.  

Suppose that the estimate of the volatility on day $n − 1$ is $1.6\%$ per day
so that $\sigma^2 = 0.0162 = 0.000256$, and on that day $n − 1$ the market
variable decreased by $1\%$ so that $y_{n−1}^2 = 0.01^2 = 0.0001$. Then
$$\sigma_n^2 = 0.000002 + 0.13 × 0.0001 + 0.86 × 0.000256 = 0.00023516$$
the new estimate of the volatility is: $\sqrt{0.00023516} = 0.0153$ or $1.53\%$ per day.

# Maximum likelihood

Maximum likelihood is the most important and widespread method of estimation. What is maximum likelihood?

Ask the question which parameters most likely generated the data we have. Suppose we have a sample of $\{−0.2, 3, 4, −1, 0.5\}$. In the following three possibilities, which is most likely for parameters?

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| case | $μ$ | $σ$ |
|------|:-----:|---------:|
| 1 | 1 | 5 |
| 2 | -2 | 2 |
| 3 | 1 | 2 |
"
cat(tabl)
```

Let $Y = (y_1,y_2,...,y_n)$ be a vector of data and let $\theta = (\theta_1,\theta_2,...,\theta_p)$ be a vector of parameters. Let $f(Y | \theta)$ be the density of Y which depends on the parameters. The function
$$L(\theta) := f(Y | \theta)$$
is viewed as the function of $\theta$ with $Y$ fixed at the observed data is called
the likelihood function.

* The maximum likelihood estimator (MLE) is the value of $\theta$ that maximizes the likelihood function. We denote the MLE by $\hat \theta_{ML}$.
* It is mathematically easier to maximize $\log L(\theta)$, which is called the log-likelihood. If the data are independent, then the likelihood is the product of the marginal densities.

## Application to ARCH(1)

Consider ARCH(1) model:

$$  εt ∼N(0,1) $$
For $t = 2$ we have the density??

$$ f(y_2|y_1)=\frac{1}{\sqrt{2\pi(\omega+\alpha y_1^2)}} e^{-\frac{1}{2} \frac{y_2^2}{2\omega+\alpha y_1^2}} $$
Hence, the joint density
$$ \prod_{t=2}^T f(y_t|y_{t-1})=\prod_{t=2}^T \frac{1}{\sqrt{2\pi(\omega+\alpha y_{t-1}^2)}} e^{-\frac{1}{2} \frac{y_t^2}{2\omega+\alpha y_{t-1}^2}} $$
and, the log likelihood
$$ \log(L(\omega, \alpha)) =-\frac{T-1}{2} \log(2\pi)-\frac{1}{2} \sum_{t=2}^T \left( \log(\omega+\alpha y_{t-1}^2) + \frac{y_t^2}{\omega+\alpha y_{t-1}^2} \right) $$

##  Application to GARCH(1,1)

$$ \sigma_t^2=\omega+\alpha y_{t-1}^2 +\beta \sigma_{t-1}^2$$
Hence, the joint density
$$ f(y_2|y_1)=\frac{1}{\sqrt{2\pi(\omega+\alpha y_1^2+\beta \hat \sigma_1^2)}} e^{-\frac{1}{2} \frac{y_2^2}{\omega +\alpha y_1^2+\beta \hat \sigma_1^2}} $$
and, the log likelihood
$$ \log(L(\omega, \alpha)) =-\frac{T-1}{2} \log(2\pi)-\frac{1}{2} \sum_{t=2}^T \left( \log(\omega+\alpha y_{t-1}^2+\beta \hat \sigma_{t-1}^2) + \frac{y_t^2}{\omega+\alpha y_{t-1}^2+\beta \hat \sigma_{t-1}^2} \right) $$

### The importance of σ1

* $\sigma_1$ can make a large difference.
* Especially when the sample size is small. 
* Typically set $\sigma_1 = \hat \sigma$.

### Volatility targeting {.unnumbered}

* Since we have the long-run variance rate 
$$\sigma^2= \frac{\omega}{1-\alpha-\beta}$$.
* We can set
$$ \omega=\hat \sigma^2(1-\alpha-\beta) $$
where $\hat \sigma^2$ is is the sample variance.
* Hence we save one parameter in the estimation.

# Future volatility

The variance rate estimated at the end of day $n − 1$ for $n$ day when apply $GARCH(1,1)$ model is

$$ \sigma_n^2=\omega+\alpha y_{n-1}^2+\beta \sigma_{n-1}^2=\sigma^2(1-\alpha-\beta)+\alpha y_{n-1}^2+\beta \sigma_{n-1}^2 $$
or
$$ \sigma_n^2- \sigma^2=\alpha (y_{n-1}^2-\sigma^2)+\beta (\sigma_{n-1}^2-\sigma^2) $$
On day $n+t$ in the future we have
$$ \sigma_{n+t}^2 - \sigma^2=\alpha (y_{n+t-1}^2-\sigma^2)+\beta(\sigma_{n+t-1}^2-\sigma^2) $$
Hence,
$$ \mathbb{E}[\sigma_{n+t}^2 - \sigma^2]=(\alpha+\beta)\mathbb{E}[\sigma_{n+t-1}^2 - \sigma^2] $$
By induction we obtain
$$ \mathbb{E}(\sigma_{n+t}^2) = \sigma^22 + (\alpha + \beta)^t(\sigma_n^2 − \sigma^2) $$

## Example {.unnumbered}

For the S&P data consider earlier, $\alpha + \beta = 0.9935$, the log-run variance rate $\sigma^2 = 0.0002075$ (or $\sigma = 1.44\%$ per day). Suppose that our estimate of the current variance rate per day is $0.0003$ (This corresponds to a volatility of $1.732\%$ per day). In $t = 10$ days, calculate the expected variance rate??  

We have $\sigma_n^2 = 0.0003$, hence
$$\begin{align*}
\mathbb{E}(\sigma_{n+10}^2 ) = 0.0002075 + 0.993510^{10} × (0.0003 − 0.0002075) = 0.0002942
\end{align*}$$
or the expected volatility per day is $\sqrt{0.0002942} = 1.72\%$, still above the long-term volatility of $1.44\%$ per day.

##  Volatility term structures {.unnumbered}

Suppose it is day $n$. We define
$$V(t) = \mathbb{E}(\sigma_{n+1}^2 )$$
and
$$ a:= \log \left( \frac{1}{\alpha+\beta} \right) $$
From $\mathbb{E}(\sigma_{n+t}^2) = \sigma^22 + (\alpha + \beta)^t(\sigma_n^2 − \sigma^2)$ we have

$$V(t) = \sigma^2 + e^{−at}(V(0) − \sigma^2)$$
Then we have the average variance rate per day between today and time T.

$$ \frac{1}{T} \int_0^T V(t) \,dt=\frac{1}{T} \int_0^T \left( \sigma^2 + e^{−at}(V(0) − \sigma^2) \right)=\sigma^2+\frac{1-e^{-aT}}{aT} [V(0)-\sigma^2] $$

Now we define $sigma(T)$ the volatility per annum that should be used to price a T-day option under $GARCH(1,1)$ model. Then we have

$$ \sigma^2(T)=252 \left( \sigma^2+\frac{1-e^{-aT}}{aT} [V(0)-\sigma^2] \right) $$
This relationship between the volatility of options and their maturities is referred to as the volatility term structure.

### Example {.unnumbered}

For S&P data, using GARCH(1,1) model we obtain the coefficients $\omega = 0.0000013465$, $\alpha = 0.083394$ and $\beta = b = 0.910116$. So from 
$$ \sigma^2(T)=252 \left( \sigma^2+\frac{1-e^{-aT}}{aT} [V(0)-\sigma^2] \right) $$
assume that $V(0) = 0.0003$ we have
$$ \sigma^2=\frac{0.0000013465}{1 − 0.083394 − 0.910116}=0.0002073 $$
and $a = \log \left( \frac{1}{0.99351} \right) = 0.00651$. Hence,
$$ \sigma^2(T)=252 \left( 0.0002073+\frac{1-e^{-0.00651 \times T}}{0.00651 \times T}[0.0003-0.0002073] \right) $$

For the option life (days) T = 10, 30, 50, 100, 500, we obtain the option
volatility ($\%$ per annum)

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| Option life (days) | 10 | 30 | 50 | 100 | 500 |
|------|:-----:|------:|------:|------:|------:|
| Option volatility | 27.36 | 27.10 | 26.87 | 26.35 | 24.32 |
"
cat(tabl)
```

# In-class exercise

## Autocovariance

1. Find the autocovariance function of Brownian motion?

>$$\begin{align*}
&B_t \sim \mathcal{N}(0,t) \\
&\rightarrow E[B_t^2] =Var(B_t)=t
\end{align*}$$

>$$\begin{align*}
&B_t-B_{t-\tau} \sim \mathcal{N}(0,t-\tau) \\
&\rightarrow E[(B_t-B_{t-\tau})^2]=Var(B_t-B_{t-\tau})=t-\tau
\end{align*}$$

>$$\begin{align*}
\gamma(t,\tau)&=E[(B_t-\mu_t)(B_{t-\tau}-\mu_{t-\tau})] \\
&=E[B_tB_{t-\tau}] \\
&=-\frac{1}{2}E[(B_t-B_{t-\tau})^2-B_t^2-B_{t-\tau}^2] \\
&=-\frac{1}{2}\{E[(B_t-B_{t-\tau})^2] -E[B_t^2]-E[B_{t-\tau}^2]\} \\
&=-\frac{1}{2} [(\tau)-(t)-(t-\tau)]=t-\tau
\end{align*}$$

>Answer: The autocovariance function of Brownian motion is $t-\tau$

2. Let $cov (X_t,X_{t+h})=\gamma(h)$

a. Prove that $\gamma(h)=\gamma(-h)$

>$$\begin{align*}
\gamma(h)=cov(X_t,X_{t+h}) \\
\rightarrow \gamma(-h)=cov(X_{t},X_{t-h})
\end{align*}$$

>Let $s=t-h$ then $t=s+h$
$$\gamma(-h)=cov(X_{s+h},X_{s})=\gamma(h) $$

b. Prove that $-1 \leq \rho(h) \le1$

>$$\begin{align*}
&\mathbb{E}[(X_{t+h} \pm X_{t})^2] \ge 0 \\ 
&\rightarrow \mathbb{E}[X_{t+h}^2] + \mathbb{E}[X_{t}^2] \pm 2 \mathbb{E}[X_{t+h}X_{t}] \ge 0 \\ 
&\rightarrow 2 \gamma(0) \pm 2\gamma(h) \ge 0 \\ 
&\rightarrow -2 \gamma(0) \leq 2\gamma(h) \leq 2 \gamma(0) \\
&\rightarrow -1 \leq \rho(h) \leq 1
\end{align*}$$

## ADF test

### Python {.unnumbered} 

```{python,warning=FALSE,message=FALSE}
import pandas_datareader as web
import numpy as np
import matplotlib.pyplot as plt

price = web.get_data_yahoo("^gspc",
start = "2009-01-01",
end = "2021-12-31")

# Log-data
x=np.log(price['Adj Close'])
plt.plot(x,color="black")
plt.show()

# First difference of log-data
y=np.diff(np.log(price['Adj Close']))
plt.plot(y,color="black")
plt.show()
```

```{python,warning=FALSE,message=FALSE}
from statsmodels.tsa.stattools import adfuller
result=adfuller(x)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
result=adfuller(y)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
```

### R {.unnumbered} 

```{r,warning=FALSE,message=FALSE}
library(tseries)
library(zoo)
price = get.hist.quote(instrument = "^gspc",start = "2009-01-01",
                       end = ("2021-12-31"),  quote = "AdjClose")
x=coredata(log(price))
y=coredata(diff(log(price)))
# Log-data
ts.plot(x,xlab="time",ylab="returns")
# First difference of log-data
ts.plot(y,xlab="time",ylab="returns")
```

```{r,warning=FALSE,message=FALSE}
library(aTSA)
adf.test(x)
adf.test(y)
```

## KPSS test

### Python {.unnumbered} 

```{python,warning=FALSE,message=FALSE}
import statsmodels.api as sm
#perform KPSS test
result=sm.tsa.stattools.kpss(x)
print('KPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
result=sm.tsa.stattools.kpss(y)
print('KPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
```

### R {.unnumbered} 

```{r,warning=FALSE,message=FALSE}
library(tseries)
kpss.test(x)
kpss.test(y)
```

## Fit ARIMA 

### Python {.unnumbered} 

```{python,warning=FALSE,message=FALSE}
from statsmodels.tsa.arima.model import ARIMA
model = ARIMA(x, order=(1,0,0))
model_fit = model.fit()
print(model_fit.summary())
model = ARIMA(y, order=(1,0,0))
model_fit = model.fit()
print(model_fit.summary())
```

### R {.unnumbered} 

```{r,warning=FALSE,message=FALSE}
fitAR1=arima(x, order = c(1,0,0))
print(fitAR1)
fitAR2=arima(y, order = c(1,0,0))
print(fitAR2)
```

## What happens with price

### Python {.unnumbered} 

```{python,warning=FALSE,message=FALSE}
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
import statsmodels.api as sm
#what happens if we fit AR for price, not that price is non-stationary
fitAR=ARIMA(price['Adj Close'], order=(1,0,0))
print(fitAR)
result=adfuller(price['Adj Close'])
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
result=sm.tsa.stattools.kpss(price['Adj Close'])
print('KPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
```

### R {.unnumbered} 

```{r,warning=FALSE,message=FALSE}
library(tseries)
library(zoo)
#what happens if we fit AR for price, not that price is non-stationary
fitAR=arima(price$Adjusted, order = c(1,0,0))
print(fitAR)
adf.test(coredata(price))
kpss.test(coredata(price))
```

# Homework

## Problem 1

Give the data

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| Time  | Data  | Time  | Data  | Time | Data |
|------:|:-----:|------:|------:|-----:|-----:|
| 1 | 10.31778 | 7 | 10.36644 | 13 | 10.34145 |
| 2 | 10.29235 | 8 | 10.36744 | 14 | 10.40247 |
| 3 | 10.30075 | 9 | 10.39553 | 15 | 10.39158 |
| 4 | 10.29208 | 10 | 10.48562 | 16 | 10.35517 |
| 5 | 10.31304 | 11 | 10.47619 | 17 | 10.35166 |
| 6 | 10.32042 | 12 | 10.46396 | 18 | 10.36395 |
"
cat(tabl)
```

Calculate the ACF with lags from 0 to 15 and lower, upper bounds with significant α = 5%

### Python {.unnumbered} 

```{python,warning=FALSE,message=FALSE}
import numpy as np
import warnings
warnings.filterwarnings("ignore")

# Import Data
data = [10.31778, 10.29235, 10.30075, 10.29208, 10.31304, 10.32042,
        10.36644, 10.36744, 10.39553, 10.48562, 10.47619, 10.46396,
        10.34145, 10.40247, 10.39158, 10.35517, 10.35166, 10.36395]
```

```{python,warning=FALSE,message=FALSE}
import pandas as pd
df = pd.DataFrame(data, columns = ['data'])
df.head(5)
```

```{python,warning=FALSE,message=FALSE}
from statsmodels.tsa.stattools import acf
# Calculate ACF
print('Calculated ACF by lags 0 - 15:\n')
acf(df['data'], nlags = 15)
```

```{python,warning=FALSE,message=FALSE}
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf
# Plot ACF
y = 1.96 / np.sqrt(len(data))
plot_acf(df['data'], lags = 15, alpha = 0.05, use_vlines = True)
plt.plot(range(0, 16), [y] * 16, 'b--', linewidth = 0.75)
plt.plot(range(0, 16), [-y] * 16, 'b--', linewidth = 0.75)
plt.show()
```

### R {.unnumbered} 

```{r,warning=FALSE,message=FALSE}
# Import Data
data = c(10.31778, 10.29235, 10.30075, 10.29208, 10.31304, 10.32042,
        10.36644, 10.36744, 10.39553, 10.48562, 10.47619, 10.46396,
        10.34145, 10.40247, 10.39158, 10.35517, 10.35166, 10.36395)
```

```{r,warning=FALSE,message=FALSE}
head(data)
```

```{r,warning=FALSE,message=FALSE}
# Calculate ACF
acf(data,lag.max = 15,plot=F)
```

```{r,warning=FALSE,message=FALSE}
# Plot ACF
acf(data,lag.max = 15)
```

## Problem 2

Consider the $MA(q)$. 

a. Calculate autocovariance $\gamma(h)$ and ACF $\rho(h)$.

>$$\begin{align*}
\gamma(0)&=Var(X_t) \\
&=Var(\omega_t)+\sum_{j=1}^{q} \theta_j^2 \times Var(\omega_{t-j}) \\
&=\sigma^2 \left(1+ \sum_{j=1}^q \theta_j^2 \right)
\end{align*}$$

>$$\begin{align*}
\gamma(h)&=Cov(X_t,X_{t-h}) \\
&=Cov \left(\omega_t+\sum_{j=1}^{q} \theta_j \omega_{t-j},\omega_{t-h}+\sum_{j=1}^{q} \theta_j \omega_{t-h-j} \right) \\
&=\sigma^2 \left(\theta_h+ \sum_{i=1}^{q-h} \theta_i \theta_{h+i} \right)
\end{align*}$$

>$$ \rho(h)=\frac{\gamma(h)}{\gamma(0)}=\begin{cases}
    1 & h=0\\
    \frac{\left(\theta_h+ \sum_{i=1}^{q-h} \theta_i \theta_{h+i} \right)}{\left(1+ \sum_{j=1}^q \theta_j^2 \right)} & 1 \leq h \leq q\\
    0 & h>q
  \end{cases} $$

>Answer: $$\begin{align*}
\gamma(h) &=\sigma^2 \left(\theta_h+ \sum_{i=1}^{q-h} \theta_i \theta_{h+i} \right) \\
\rho(h) &=\begin{cases}
    1, & h=0.\\
    \frac{\left(\theta_h+ \sum_{i=1}^{q-h} \theta_i \theta_{h+i} \right)}{\left(1+ \sum_{j=1}^q \theta_j^2 \right)}, & 1 \leq h \leq q.\\
    0, & h>q.
  \end{cases}
  \end{align*}$$

b. Show that the MA(1) processes $X_t$ and $Y_t$
$$\begin{align*}
X_t = \beta \epsilon_{t−1} + \epsilon_t \\ 
Y_t = \frac{1}{\beta} \epsilon_{t−1} + \epsilon_t \\ 
\end{align*}$$
have the same ACF.

>Consider the $MA(1)$ model $X_t=\beta \epsilon_{t-1}+\epsilon_t$. The coefficient $\theta_1=\beta$. The ACF is given by
$$ \rho_1=\frac{\beta}{1+\beta^2}, (1) $$
and $\rho_h=0, \forall h \geq 2$ 

>Consider the $MA(1)$ model $Y_t=\frac{1}{\beta} \epsilon_{t-1}+\epsilon_t$. The coefficient $\theta_1=\frac{1}{\beta}$. The ACF is given by
$$\begin{align*} \rho_1&=\frac{\frac{1}{\beta}}{1+\left(\frac{1}{\beta}\right)^2} \\
&=\frac{1}{\beta} \times \frac{\beta^2}{1+\beta^2} \\
&=\frac{\beta}{1+\beta^2}, (2) 
\end{align*}$$
and $\rho_h=0, \forall h \geq 2$ 

>(1),(2) imply two $MA(1)$ processes above have the same ACF.

>Answer: $X_t$ and $Y_t$ have the same ACF.

## Problem 3

Suppose that the stationary process $X_t$ has an autocovariance function given by $\gamma(h)$.
Consider process $Y_t = X_t − X_{t−1}$.

a) Is the process $Y_t$ stationary

>$$\begin{align*}
\mathbb{E}(Y_t)&=\mathbb{E}(X_t)-\mathbb{E}(X_{t-1}) \\
&=0 \\
Var(Y_t) &=Var(X_t-X_{t=1}) \\
&=Var(X_t)+Var(x_{t-1})-2Cov(X_t,X_{t-1}) \\
&=2\sigma_x^2-2\gamma_x(1) < \infty \\
Cov(Y_t,Y_{t-k})&=Cov(X_t-X_{t-1},X_{t-k}-X_{t-k-1}) \\
&=Cov(X_t,X_{t-k})-Cov(X_{t-1},X_{t-k})-Cov(X_t,X_{t-k-1})+Cov(X_{t-1},X_{t-k-1}) \\
&=2\gamma_x(k)-\gamma_x(k-1-\gamma_x(k+1)) \\
&=\gamma_y(k)
\end{align*}$$

>Answer: $\{Yt\}$ is a weakly stationary proces

b) Find the autocorrelation function of $Y_t$

>$$ \rho_y(h)=\frac{\gamma_y(h)}{\gamma_y(0)}=\frac{2\gamma_x(k)-\gamma_x(k-1-\gamma_x(k+1))}{2 \sigma_x^2-2\gamma_x(1)} $$

>Answer: The autocorrelation function of $Y_t$ is $\frac{2\gamma_x(k)-\gamma_x(k-1-\gamma_x(k+1))}{2 \sigma_x^2-2\gamma_x(1)}$

## Problem 4

Find the autocorrelation function of the second order moving average process $MA(2)$
$$X_t = 0.5 \epsilon_{t−1} − 0.2 \epsilon_{t−2} + \epsilon_t$$
where $\epsilon_t$ is the white noise.

>$$\begin{align*}
\gamma(0)&=Var(X_t)=0.5^2 \sigma^2 +0.2^2 \sigma^2+\sigma^2 \\
&=1.29 \sigma^2 \\
\gamma(1) &=Cov(X_t,X_{t-1}) \\
&=Cov(0.5 \epsilon_{t-1} - 0.2 \epsilon_{t-2}+\epsilon_t,0.5 \epsilon_{t-2} -0.2\epsilon_{t-3}+\epsilon_{t-1}) \\
&=0.5Var(\epsilon_{t-1})-0.1Var(\epsilon_{t-2}) \\
&=0.4 \sigma^2 \\
\gamma(2) &=Cov(X_t,X_{t-2}) \\
&=Cov(0.5 \epsilon_{t-1} - 0.2 \epsilon_{t-2}+\epsilon_t,0.5 \epsilon_{t-3} -0.2\epsilon_{t-4}+\epsilon_{t-2}) \\
&=-0.2Var(\epsilon_{t-2}) \\
&=-0.2 \sigma^2
\end{align*}$$

>$$\rho(h)=\begin{cases}
    1&  h=0.\\
    0.31&  h=1\\
    -0.155&  h=2 \\
    0&  h \geq 3
  \end{cases}$$
  
>Answer: The autocorrelation function of the second order moving average process is
$$\rho(h)=\begin{cases}
    1&  h=0.\\
    0.31&  h=1\\
    -0.155&  h=2 \\
    0&  h \geq 3
  \end{cases}$$